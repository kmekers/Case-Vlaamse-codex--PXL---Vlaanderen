{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vlaamse codex notebook\n",
    "\n",
    "Deze notebook is gemaakt vanuit de opleiding PXL business architect AI. Voor de case rond de **Vlaamse Codex**\n",
    "\n",
    "Teamleden: Koen, Charlotte, Kim\n",
    "\n",
    "Coach: Tim\n",
    "\n",
    "Code door Koen Mekers (functioneel analist, geen dev dus de code is niet helemaal optimaal)\n",
    "\n",
    "Special thanks to: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deze cell is de scraper cell. Hier wordt de volledige codex ingehaald in Jsonformaat.\n",
    "Er is een mogelijkheid om de volledige codex in te halen, of te kiezen voor een select aantal. Duurt ca. 40 min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vlaamse codex API notebook\n",
    "import requests\n",
    "import os\n",
    "import time \n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm # Voor progress bar (I know, it's awesome)\n",
    "\n",
    "# Configuratie\n",
    "OUTPUT_DIR = \"codexjson\"\n",
    "ERROR_LOG = \"error_log.txt\"\n",
    "BASE_URL = \"https://codex.opendata.api.vlaanderen.be/api/WetgevingDocument\"\n",
    "\n",
    "# Zorg dat de output directory bestaat\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Maak een session object aan voor hergebruik van verbindingen => epic scrapesnelheid\n",
    "SESSION = requests.Session()\n",
    "\n",
    "def log_error(message):\n",
    "    \"\"\"Log een error met timestamp.\"\"\"\n",
    "    with open(ERROR_LOG, 'a', encoding='utf-8') as f: # encoding toegevoegd\n",
    "        f.write(f\"{datetime.now()} - {message}\\n\")\n",
    "\n",
    "def fetch_document(session, doc_id, retries=3, error_delay=1): # error_delay voor algemene fouten\n",
    "    \"\"\"\n",
    "    Haal de JSON-data op voor een gegeven document-ID.\n",
    "    Probeert maximaal 'retries' keer bij falen.\n",
    "    Bij status 429 (rate limit) wordt dit gelogd, maar niet lang gewacht.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/{doc_id}/VolledigDocument\"\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = session.get(url, timeout=20) # Timeout voor het request\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    return response.json()\n",
    "                except json.JSONDecodeError as e_json:\n",
    "                    log_error(f\"Attempt {attempt}: JSONDecodeError for document {doc_id} (status 200). Error: {str(e_json)}. Response text: {response.text[:200]}\")\n",
    "                    time.sleep(error_delay) # Korte wacht bij JSON parse error\n",
    "            elif response.status_code == 429: # Rate limit, gaat niet gebeuren, maar toch.\n",
    "                log_error(f\"Attempt {attempt}: Rate limit (429) hit for document {doc_id}. API returned 429, proceeding with minimal delay.\")\n",
    "                time.sleep(0.1) # Minimale pauze\n",
    "            else: # Andere HTTP errors (404, 500, etc.)\n",
    "                log_error(f\"Attempt {attempt}: HTTP {response.status_code} for document {doc_id}. Response: {response.text[:200]}. Retrying after {error_delay}s.\")\n",
    "                time.sleep(error_delay)\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            log_error(f\"Attempt {attempt}: Timeout for document {doc_id}. Retrying after {error_delay}s.\")\n",
    "            time.sleep(error_delay)\n",
    "        except requests.exceptions.ConnectionError as e_conn:\n",
    "            log_error(f\"Attempt {attempt}: ConnectionError for document {doc_id}: {str(e_conn)}. Retrying after {error_delay}s.\")\n",
    "            time.sleep(error_delay)\n",
    "        except requests.exceptions.RequestException as e_req: # Vangt andere requests-gerelateerde exceptions\n",
    "            log_error(f\"Attempt {attempt}: RequestException for document {doc_id}: {str(e_req)}. Retrying after {error_delay}s.\")\n",
    "            time.sleep(error_delay)\n",
    "        except Exception as e_generic:\n",
    "             log_error(f\"Attempt {attempt}: Unexpected generic exception for document {doc_id}: {type(e_generic).__name__} - {str(e_generic)}. Retrying after {error_delay}s.\")\n",
    "             time.sleep(error_delay)\n",
    "\n",
    "    log_error(f\"Failed to fetch document {doc_id} after {retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "def get_documents(session, full_codex=False, start_id=None, num_docs=None):\n",
    "    \"\"\"\n",
    "    Verwerkt documenten:\n",
    "      - full_codex=True: haal de volledige lijst op via paginatie (met top/skip).\n",
    "      - anders: verwerk een ID-range.\n",
    "    \"\"\"\n",
    "    if full_codex:\n",
    "        documents_list_meta = []\n",
    "        top = 200  # Aantal resultaten per pagina\n",
    "        skip = 0   # Offset\n",
    "        page_num = 1\n",
    "\n",
    "        print(\"Fetching document metadata list...\")\n",
    "        while True:\n",
    "            params = {\"top\": top, \"skip\": skip}\n",
    "            request_url = f\"{BASE_URL}?top={top}&skip={skip}\"\n",
    "            try:\n",
    "                # Gebruik session & timeout voor het ophalen van de lijst\n",
    "                response = session.get(BASE_URL, params=params, timeout=30)\n",
    "                response.raise_for_status()  # Genereert een HTTPError voor 4xx/5xx status codes\n",
    "                data = response.json()\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                print(f\"Stopping. HTTP error fetching document list (page {page_num}, skip {skip}): {e}\")\n",
    "                log_error(f\"HTTPError fetching document list: {e}. URL: {e.request.url if e.request else request_url}\")\n",
    "                break \n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"Stopping. Timeout fetching document list (page {page_num}, skip {skip}): {e}\")\n",
    "                log_error(f\"Timeout fetching document list: {e}. URL: {request_url}\")\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Stopping. Error fetching document list (page {page_num}, skip {skip}): {e}\")\n",
    "                log_error(f\"RequestException fetching document list: {e}. URL: {request_url}\")\n",
    "                break\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Stopping. Error decoding JSON from document list (page {page_num}, skip {skip}): {e}\")\n",
    "                log_error(f\"JSONDecodeError fetching document list: {e}. Response text: {response.text[:200] if 'response' in locals() and hasattr(response, 'text') else 'N/A'}\")\n",
    "                break\n",
    "\n",
    "            docs_page = data.get(\"ResultatenLijst\", [])\n",
    "            if not docs_page:\n",
    "                print(\"No more document entries found in the list.\")\n",
    "                break \n",
    "\n",
    "            documents_list_meta.extend(docs_page)\n",
    "            print(f\"Page {page_num}: Fetched {len(docs_page)} document entries. Total metadata entries: {len(documents_list_meta)}\")\n",
    "            skip += top\n",
    "            page_num += 1\n",
    "            # time.sleep(0.1) # Verwijderd voor \"full gass\"\n",
    "\n",
    "        if not documents_list_meta:\n",
    "            print(\"No document metadata was fetched. Exiting.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nTotal document metadata entries found: {len(documents_list_meta)}\")\n",
    "        print(\"Now fetching full documents...\")\n",
    "\n",
    "        # Gebruik tqdm voor een progress bar \n",
    "        for doc_meta in tqdm(documents_list_meta, desc=\"Fetching full documents\", unit=\"doc\"):\n",
    "            doc_id = doc_meta.get(\"Id\")\n",
    "            if not doc_id:\n",
    "                log_error(f\"Document metadata missing 'Id': {doc_meta}\")\n",
    "                continue\n",
    "            \n",
    "            doc_data = fetch_document(session, doc_id) # Geef session door\n",
    "            if doc_data:\n",
    "                file_path = os.path.join(OUTPUT_DIR, f\"document_{doc_id}_raw.json\")\n",
    "                try:\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(doc_data, f, indent=2, ensure_ascii=False)\n",
    "                except IOError as e_io:\n",
    "                    log_error(f\"IOError writing document {doc_id} to {file_path}: {e_io}\")\n",
    "                except Exception as e_file:\n",
    "                    log_error(f\"Unexpected error writing document {doc_id} to {file_path}: {e_file}\")\n",
    "            # else: fetch_document logt zelf al fouten als het None retourneert\n",
    "\n",
    "    else: # ID-range mode\n",
    "        if start_id is None or num_docs is None or not isinstance(num_docs, int) or num_docs <= 0:\n",
    "            msg = \"Error: start_id and a positive integer num_docs must be provided when full_codex is False.\"\n",
    "            print(msg)\n",
    "            log_error(msg)\n",
    "            return\n",
    "\n",
    "        print(f\"Fetching {num_docs} documents starting from ID {start_id}...\")\n",
    "        # Gebruik tqdm voor een progress bar\n",
    "        for i in tqdm(range(num_docs), desc=\"Fetching documents by ID range\", unit=\"doc\"):\n",
    "            current_id = start_id + i\n",
    "            doc_data = fetch_document(session, current_id) # Geef session door\n",
    "            if doc_data:\n",
    "                file_path = os.path.join(OUTPUT_DIR, f\"document_{current_id}_raw.json\")\n",
    "                try:\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(doc_data, f, indent=2, ensure_ascii=False)\n",
    "                except IOError as e_io:\n",
    "                    log_error(f\"IOError writing document {current_id} to {file_path}: {e_io}\")\n",
    "                except Exception as e_file:\n",
    "                    log_error(f\"Unexpected error writing document {current_id} to {file_path}: {e_file}\")\n",
    "            # else: fetch_document logt zelf al fouten\n",
    "\n",
    "# Configuratie voor het ophalen van documenten\n",
    "START_ID = 10001   # Pas aan indien nodig\n",
    "NUM_DOCS = 40000   # Aantal documenten om op te halen als FULL_CODEX False is\n",
    "FULL_CODEX = True  # Zet op True om de volledige codex te proberen, False voor een ID range\n",
    "\n",
    "# Start het ophalen van documenten\n",
    "\n",
    "get_documents(\n",
    "    session=SESSION, # Geef het session object mee\n",
    "    full_codex=FULL_CODEX,\n",
    "    start_id=START_ID,\n",
    "    num_docs=NUM_DOCS\n",
    ")\n",
    "\n",
    "print(f\"Script finished. Check '{OUTPUT_DIR}' for documents and '{ERROR_LOG}' for any errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing ID + check html in Json\n",
    "\n",
    "Het viel op dat er missing ID's waren. Bovendien waren er Json's die HTML bevatten. Moest je willen omzetten naar embeddings of markdown, dan zal je de data moeten cleanen. Dit is ook de reden waarom ik ervoor gekozen heb om de documenten te scrapen via HTML (zie later) op de website omdat dit minder werk was. Bovendien zijn er voor een of andere reden absoluut geen rate limits. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# --- Configuratie ---\n",
    "DATA_DIRECTORY = \"codexjson\"  # De map waar je gescrapete JSON-bestanden staan\n",
    "REPORTS_DIRECTORY = \"reports\"\n",
    "\n",
    "# Zorg ervoor dat de rapportenmap bestaat\n",
    "os.makedirs(REPORTS_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# --- Functie om ontbrekende documentnummers te vinden ---\n",
    "def find_missing_document_numbers(directory):\n",
    "    \"\"\"\n",
    "    Vindt ontbrekende documentnummers in een gegeven map.\n",
    "    Bestanden moeten het patroon 'document_{nummer}_raw.json' volgen.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(directory) if f.startswith(\"document_\") and f.endswith(\"_raw.json\")]\n",
    "\n",
    "    numbers = []\n",
    "    for file in files:\n",
    "        match = re.search(r'document_(\\d+)_raw\\.json', file)\n",
    "        if match:\n",
    "            numbers.append(int(match.group(1)))\n",
    "\n",
    "    if not numbers:\n",
    "        return {\n",
    "            \"first_number\": None,\n",
    "            \"last_number\": None,\n",
    "            \"total_files_found\": 0,\n",
    "            \"missing_numbers\": [],\n",
    "            \"missing_count\": 0,\n",
    "            \"directory_scanned\": directory\n",
    "        }\n",
    "\n",
    "    numbers.sort()\n",
    "    missing = []\n",
    "    if numbers: # Alleen als er nummers zijn gevonden\n",
    "        for i in range(numbers[0], numbers[-1] + 1):\n",
    "            if i not in numbers:\n",
    "                missing.append(i)\n",
    "\n",
    "    return {\n",
    "        \"first_number\": numbers[0] if numbers else None,\n",
    "        \"last_number\": numbers[-1] if numbers else None,\n",
    "        \"total_files_found\": len(numbers),\n",
    "        \"missing_numbers\": missing,\n",
    "        \"missing_count\": len(missing),\n",
    "        \"directory_scanned\": directory\n",
    "    }\n",
    "\n",
    "def save_missing_report_to_json(data, report_dir):\n",
    "    \"\"\"\n",
    "    Slaat het rapport over ontbrekende documenten op als JSON.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(report_dir, \"missing_document_numbers.json\")\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Rapport over ontbrekende documenten opgeslagen in: {filepath}\")\n",
    "\n",
    "# --- Functie om HTML in JSON-bestanden te detecteren ---\n",
    "def find_documents_with_html(directory):\n",
    "    \"\"\"\n",
    "    Detecteert JSON-bestanden in een map die HTML-tags lijken te bevatten.\n",
    "    \"\"\"\n",
    "    files_with_html = []\n",
    "    # Regex om te zoeken naar veelvoorkomende HTML-tags (case-insensitive)\n",
    "    # Dit is een heuristiek en kan valse positieven/negatieven hebben.\n",
    "    html_pattern = re.compile(r\"<[^>]+>\", re.IGNORECASE)\n",
    "\n",
    "    json_files = [f for f in os.listdir(directory) if f.startswith(\"document_\") and f.endswith(\"_raw.json\")]\n",
    "\n",
    "    for filename in json_files:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content_str = f.read() # Lees het hele bestand als string\n",
    "                \n",
    "                # Simpele check op HTML-tags in de string-representatie\n",
    "                if html_pattern.search(content_str):\n",
    "                    # Probeer het document ID te extraheren voor een schonere lijst\n",
    "                    match = re.search(r'document_(\\d+)_raw\\.json', filename)\n",
    "                    if match:\n",
    "                        files_with_html.append(int(match.group(1)))\n",
    "                    else:\n",
    "                        files_with_html.append(filename) # Fallback naar bestandsnaam\n",
    "        except Exception as e:\n",
    "            print(f\"Fout bij het lezen of verwerken van bestand {filepath}: {e}\")\n",
    "            # Optioneel: log deze fout naar je error_log.txt\n",
    "            # log_error(f\"Fout bij HTML-check voor {filepath}: {e}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"documents_containing_html\": sorted(list(set(files_with_html))), # Sorteer en maak uniek\n",
    "        \"count\": len(set(files_with_html)),\n",
    "        \"directory_scanned\": directory\n",
    "    }\n",
    "\n",
    "def save_html_report_to_json(data, report_dir):\n",
    "    \"\"\"\n",
    "    Slaat het rapport over documenten met HTML op als JSON.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(report_dir, \"documents_with_html.json\")\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Rapport over documenten met HTML opgeslagen in: {filepath}\")\n",
    "\n",
    "\n",
    "# --- Voer de analyses uit en sla rapporten op ---\n",
    "\n",
    "print(\"--- Analyse van Ontbrekende Documenten ---\")\n",
    "missing_docs_result = find_missing_document_numbers(DATA_DIRECTORY)\n",
    "save_missing_report_to_json(missing_docs_result, REPORTS_DIRECTORY)\n",
    "\n",
    "print(f\"Directory gescand: {missing_docs_result['directory_scanned']}\")\n",
    "print(f\"Eerste documentnummer gevonden: {missing_docs_result['first_number']}\")\n",
    "print(f\"Laatste documentnummer gevonden: {missing_docs_result['last_number']}\")\n",
    "print(f\"Totaal aantal bestanden gevonden: {missing_docs_result['total_files_found']}\")\n",
    "if missing_docs_result['missing_count'] > 0:\n",
    "    print(f\"Aantal ontbrekende documentnummers: {missing_docs_result['missing_count']}\")\n",
    "    # print(f\"Ontbrekende nummers: {missing_docs_result['missing_numbers']}\") # Kan lang zijn, optioneel\n",
    "else:\n",
    "    print(\"Geen ontbrekende documentnummers gevonden in de reeks.\")\n",
    "\n",
    "print(\"\\n--- Analyse van HTML in Documenten ---\")\n",
    "html_docs_result = find_documents_with_html(DATA_DIRECTORY)\n",
    "save_html_report_to_json(html_docs_result, REPORTS_DIRECTORY)\n",
    "\n",
    "print(f\"Directory gescand: {html_docs_result['directory_scanned']}\")\n",
    "print(f\"Aantal documenten met vermoedelijke HTML: {html_docs_result['count']}\")\n",
    "if html_docs_result['count'] > 0:\n",
    "    # print(f\"Document ID's met HTML: {html_docs_result['documents_containing_html']}\") # Kan lang zijn\n",
    "    pass\n",
    "\n",
    "\n",
    "print(f\"\\nAnalyses voltooid. Rapporten zijn te vinden in de map '{REPORTS_DIRECTORY}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kabouter plot\n",
    "\n",
    "Welke data kan ik hier vinden in mijn melkherberg? Deze cell maakt 4 plots op basis van de codexjson map die werd aangemaakt in de eerste cell. We keken naar het aantal documenten over de jaren heen en het aantal artikels. Er komt gemiddeld meer wetgeving bij. Zowel op document, als artikel niveau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DATA_DIRECTORY = \"codexjson\"\n",
    "HISTORICAL_START_DOC = 1980\n",
    "HISTORICAL_END_DOC = 2024\n",
    "FORECAST_START_DOC = 2025\n",
    "FORECAST_END_DOC = 2030\n",
    "\n",
    "# --- Helperfunctie voor het laden van jaartallen (documentniveau) ---\n",
    "def load_document_years(directory, start_yr=None, end_yr=None):\n",
    "    years_loaded = []\n",
    "    file_list = glob.glob(os.path.join(directory, '*.json'))\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if not data:\n",
    "                continue\n",
    "            document_object = data.get('Document')\n",
    "            bs_date_value = None\n",
    "            if isinstance(document_object, dict):\n",
    "                bs_date_value = document_object.get('BSDatum')\n",
    "            if bs_date_value:\n",
    "                try:\n",
    "                    year_str = str(bs_date_value).strip()[:4]\n",
    "                    year = int(year_str)\n",
    "                    if start_yr and year < start_yr:\n",
    "                        continue\n",
    "                    if end_yr and year > end_yr:\n",
    "                        continue\n",
    "                    years_loaded.append(year)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "        except Exception:\n",
    "            continue\n",
    "    return years_loaded\n",
    "\n",
    "# --- Functie voor het plotten van publicatiedata (documentniveau) ---\n",
    "def plot_document_publications(years_data, title_suffix):\n",
    "    plt.figure(figsize=(12, 7)) # Nieuwe figuur voor elke plot\n",
    "    ax = plt.gca() # Haal de current axes op\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    if not years_data:\n",
    "        ax.text(0.5, 0.5, \"Geen data gevonden\", ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(f\"Publicaties in Vlaamse Codex (documentniveau)\\n{title_suffix}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    year_counts = pd.Series(years_data).value_counts().sort_index()\n",
    "    if year_counts.empty:\n",
    "        ax.text(0.5, 0.5, \"Geen data na tellen\", ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(f\"Publicaties in Vlaamse Codex (documentniveau)\\n{title_suffix}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    mean_val = year_counts.mean()\n",
    "    sns.lineplot(x=year_counts.index, y=year_counts.values, marker='o', ax=ax, label='Documenten per jaar')\n",
    "    x_numeric = year_counts.index.astype(float)\n",
    "    y_numeric = year_counts.values.astype(float)\n",
    "\n",
    "    if len(x_numeric) >= 2:\n",
    "        sns.regplot(x=x_numeric, y=y_numeric, scatter=False, ax=ax, color='green', ci=None, line_kws={'label': 'Lineaire trend', 'linewidth': 1, 'alpha': 0.8})\n",
    "    \n",
    "    ax.axhline(y=mean_val, color='red', linestyle='--', linewidth=2.5, alpha=0.9, label=f'Gemiddeld ({mean_val:.1f} doc/jaar)')\n",
    "    y_max_val = max(year_counts.max(), mean_val) if not year_counts.empty else mean_val\n",
    "    ax.set_ylim(0, y_max_val * 1.15 if y_max_val > 0 else 10)\n",
    "    ax.set_title(f\"Publicaties in Vlaamse Codex (documentniveau)\\n{title_suffix}\", fontsize=14, pad=10)\n",
    "    ax.set_xlabel('Jaar', fontsize=12)\n",
    "    ax.set_ylabel('Aantal documenten', fontsize=12)\n",
    "    if not year_counts.empty:\n",
    "        tick_years = year_counts.index.unique().astype(int)\n",
    "        ax.set_xticks(tick_years)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Functie voor plot 3: Documenten historisch & prognose ---\n",
    "def plot_document_forecast(years_hist_data):\n",
    "    plt.figure(figsize=(12, 7)) # Nieuwe figuur\n",
    "    ax = plt.gca()\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    title_str = f\"Publicaties Vlaamse Codex (documentniveau)\\nHistorisch {HISTORICAL_START_DOC}-{HISTORICAL_END_DOC} & Prognose {FORECAST_START_DOC}-{FORECAST_END_DOC}\"\n",
    "\n",
    "    if not years_hist_data:\n",
    "        ax.text(0.5, 0.5, \"Geen historische data\", ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(title_str)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    year_counts_hist = pd.Series(years_hist_data).value_counts().sort_index()\n",
    "    if year_counts_hist.empty or len(year_counts_hist.index) < 2:\n",
    "        ax.text(0.5, 0.5, \"Te weinig data voor prognose\", ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(title_str)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    x_hist = year_counts_hist.index.astype(float)\n",
    "    y_hist = year_counts_hist.values.astype(float)\n",
    "    slope, intercept = np.polyfit(x_hist, y_hist, 1)\n",
    "    \n",
    "    full_years_range = np.arange(int(x_hist.min()), FORECAST_END_DOC + 1)\n",
    "    predicted_counts_full = slope * full_years_range + intercept\n",
    "    predicted_counts_full[predicted_counts_full < 0] = 0\n",
    "    \n",
    "    future_years_range = np.arange(FORECAST_START_DOC, FORECAST_END_DOC + 1)\n",
    "    forecast_counts_values = slope * future_years_range + intercept\n",
    "    forecast_counts_values[forecast_counts_values < 0] = 0\n",
    "    mean_hist = year_counts_hist.mean()\n",
    "\n",
    "    sns.lineplot(x=year_counts_hist.index, y=year_counts_hist.values, marker='o', ax=ax, label='Documenten per jaar (historisch)')\n",
    "    ax.plot(full_years_range, predicted_counts_full, color='green', linewidth=1, alpha=0.8, label='Lineaire trend & prognose')\n",
    "    \n",
    "    plot_future_years = future_years_range[future_years_range >= FORECAST_START_DOC]\n",
    "    plot_future_counts = forecast_counts_values[future_years_range >= FORECAST_START_DOC]\n",
    "    if plot_future_years.size > 0:\n",
    "        ax.plot(plot_future_years, plot_future_counts, color='darkorange', linestyle='--', linewidth=2, label=f'Prognose ({plot_future_years.min()}-{plot_future_years.max()})')\n",
    "\n",
    "    ax.axhline(y=mean_hist, color='red', linestyle='--', linewidth=2.5, alpha=0.9, label=f'Gemiddeld historisch ({mean_hist:.1f} doc/jaar)')\n",
    "    y_max_plot_fc = max(year_counts_hist.max() if not year_counts_hist.empty else 0, mean_hist if mean_hist else 0)\n",
    "    if predicted_counts_full.size > 0: y_max_plot_fc = max(y_max_plot_fc, predicted_counts_full.max())\n",
    "    ax.set_ylim(0, y_max_plot_fc * 1.15 if y_max_plot_fc > 0 else 10)\n",
    "    ax.set_title(title_str, fontsize=14, pad=10)\n",
    "    ax.set_xlabel('Jaar', fontsize=12)\n",
    "    ax.set_ylabel('Aantal documenten', fontsize=12)\n",
    "    \n",
    "    combined_ticks_fc = np.unique(np.concatenate((year_counts_hist.index.astype(int), future_years_range[future_years_range >= year_counts_hist.index.min()]))).astype(int)\n",
    "    ax.set_xticks(combined_ticks_fc)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Functies voor plot 4: Artikelniveau ---\n",
    "def count_articles_in_document(data):\n",
    "    if \"Inhoud\" in data and data[\"Inhoud\"] and \\\n",
    "       \"Artikelen\" in data[\"Inhoud\"] and data[\"Inhoud\"][\"Artikelen\"] is not None:\n",
    "        if isinstance(data[\"Inhoud\"][\"Artikelen\"], list):\n",
    "            return len(data[\"Inhoud\"][\"Artikelen\"])\n",
    "        else:\n",
    "            return 0 \n",
    "    count = 0\n",
    "    if \"Inhoudstafel\" in data and data[\"Inhoudstafel\"] and \\\n",
    "       \"Items\" in data[\"Inhoudstafel\"] and isinstance(data[\"Inhoudstafel\"][\"Items\"], list):\n",
    "        for item in data[\"Inhoudstafel\"][\"Items\"]:\n",
    "            if isinstance(item, dict) and item.get(\"ArtikelType\") == \"ART.\":\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def plot_artikels_per_jaar_with_forecast(directory=DATA_DIRECTORY, HISTORICAL_START=1990, HISTORICAL_END=2024, FORECAST_START=2025, FORECAST_END=2030):\n",
    "    plt.figure(figsize=(12, 7)) # Nieuwe figuur\n",
    "    ax = plt.gca()\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    title_str = f'Aantal Artikels per Jaar\\n(historisch: {HISTORICAL_START}-{HISTORICAL_END}, prognose: {FORECAST_START}-{FORECAST_END})'\n",
    "    articles_by_year = {}\n",
    "\n",
    "    for file_path in glob.glob(os.path.join(directory, \"*.json\")):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            document = data.get(\"Document\", {})\n",
    "            bs_datum = document.get(\"BSDatum\")\n",
    "            if not bs_datum:\n",
    "                continue\n",
    "            year = int(bs_datum[:4])\n",
    "            if HISTORICAL_START <= year <= HISTORICAL_END:\n",
    "                count = count_articles_in_document(data)\n",
    "                articles_by_year[year] = articles_by_year.get(year, 0) + count\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if not articles_by_year:\n",
    "        ax.text(0.5, 0.5, f\"Geen artikeldata {HISTORICAL_START}-{HISTORICAL_END}\", ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(title_str)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    series = pd.Series(articles_by_year).sort_index()\n",
    "    if series.empty or len(series.index) < 2:\n",
    "        ax.text(0.5, 0.5, \"Te weinig artikeldata voor prognose\", ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(title_str)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    mean_count = series.mean()\n",
    "    x = np.array(series.index)\n",
    "    y = np.array(series.values)\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    \n",
    "    full_years = np.arange(int(x.min()), FORECAST_END + 1) \n",
    "    predicted_counts = slope * full_years + intercept\n",
    "    predicted_counts[predicted_counts < 0] = 0\n",
    "    \n",
    "    future_years = np.arange(FORECAST_START, FORECAST_END + 1)\n",
    "    forecast_counts = slope * future_years + intercept\n",
    "    forecast_counts[forecast_counts < 0] = 0\n",
    "\n",
    "    sns.lineplot(x=series.index, y=series.values, marker='o', ax=ax, label='Artikels per jaar (historisch)')\n",
    "    ax.plot(full_years, predicted_counts, color='green', linewidth=1, alpha=0.8, label='Lineaire trend en prognose')\n",
    "    \n",
    "    plot_future_years_art = future_years[future_years >= FORECAST_START]\n",
    "    plot_future_counts_art = forecast_counts[future_years >= FORECAST_START]\n",
    "\n",
    "    if plot_future_years_art.size > 0:\n",
    "        ax.plot(plot_future_years_art, plot_future_counts_art, color='darkorange', linestyle='--', linewidth=2, label=f'Prognose ({plot_future_years_art.min()}-{plot_future_years_art.max()})')\n",
    "    \n",
    "    ax.axhline(y=mean_count, color='red', linestyle='--', linewidth=2.5, alpha=0.9, label=f'Gemiddeld ({mean_count:.1f} artikels/jaar)')\n",
    "    y_max = max(series.max() if not series.empty else 0, mean_count if mean_count else 0)\n",
    "    if predicted_counts.size > 0 : y_max = max(y_max, predicted_counts.max())\n",
    "    ax.set_ylim(0, y_max * 1.15 if y_max > 0 else 10)\n",
    "    ax.set_title(title_str, fontsize=14, pad=10)\n",
    "    ax.set_xlabel('Jaar', fontsize=12)\n",
    "    ax.set_ylabel('Totaal aantal artikels', fontsize=12)\n",
    "    \n",
    "    combined_ticks_art = np.unique(np.concatenate((series.index.astype(int), future_years[future_years >= series.index.min()]))).astype(int)\n",
    "    ax.set_xticks(combined_ticks_art)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Genereer de 4 plots ---\n",
    "\n",
    "# Plot 1\n",
    "print(\"--- Plot 1: Documenten 1980 - 2024 ---\")\n",
    "years_plot1 = load_document_years(DATA_DIRECTORY, start_yr=1980, end_yr=2024)\n",
    "plot_document_publications(years_plot1, \"1980 - 2024\")\n",
    "\n",
    "# Plot 2\n",
    "print(\"\\n--- Plot 2: Documenten 2000 - 2024 ---\")\n",
    "years_plot2 = load_document_years(DATA_DIRECTORY, start_yr=2000, end_yr=2024)\n",
    "plot_document_publications(years_plot2, \"2000 - 2024\")\n",
    "\n",
    "# Plot 3\n",
    "print(f\"\\n--- Plot 3: Documenten Historisch {HISTORICAL_START_DOC}-{HISTORICAL_END_DOC} & Prognose {FORECAST_START_DOC}-{FORECAST_END_DOC} ---\")\n",
    "years_hist_plot3 = load_document_years(DATA_DIRECTORY, start_yr=HISTORICAL_START_DOC, end_yr=HISTORICAL_END_DOC)\n",
    "plot_document_forecast(years_hist_plot3)\n",
    "\n",
    "# Plot 4\n",
    "HISTORICAL_START_ART = 1990 \n",
    "HISTORICAL_END_ART = 2024\n",
    "FORECAST_START_ART = 2025\n",
    "FORECAST_END_ART = 2030\n",
    "print(f\"\\n--- Plot 4: Artikelen Historisch {HISTORICAL_START_ART}-{HISTORICAL_END_ART} & Prognose {FORECAST_START_ART}-{FORECAST_END_ART} ---\")\n",
    "plot_artikels_per_jaar_with_forecast(directory=DATA_DIRECTORY, \n",
    "                                     HISTORICAL_START=HISTORICAL_START_ART, \n",
    "                                     HISTORICAL_END=HISTORICAL_END_ART, \n",
    "                                     FORECAST_START=FORECAST_START_ART, \n",
    "                                     FORECAST_END=FORECAST_END_ART)\n",
    "\n",
    "# --- Print voorspellingsdetails (optioneel) ---\n",
    "\n",
    "# Documenten\n",
    "print(f\"\\n--- Voorspellingsdetails Documenten (Historisch: {HISTORICAL_START_DOC}-{HISTORICAL_END_DOC}, Prognose: {FORECAST_START_DOC}-{FORECAST_END_DOC}) ---\")\n",
    "if years_hist_plot3: # Hergebruik de data geladen voor plot 3\n",
    "    year_counts_hist_doc = pd.Series(years_hist_plot3).value_counts().sort_index()\n",
    "    if not year_counts_hist_doc.empty and len(year_counts_hist_doc.index) >= 2:\n",
    "        x_hist_doc = year_counts_hist_doc.index.astype(float)\n",
    "        y_hist_doc = year_counts_hist_doc.values.astype(float)\n",
    "        slope_doc, intercept_doc = np.polyfit(x_hist_doc, y_hist_doc, 1)\n",
    "        future_years_doc_range = np.arange(FORECAST_START_DOC, FORECAST_END_DOC + 1)\n",
    "        forecast_counts_doc_values = slope_doc * future_years_doc_range + intercept_doc\n",
    "        forecast_counts_doc_values[forecast_counts_doc_values < 0] = 0\n",
    "        for year_fc, count_fc in zip(future_years_doc_range, forecast_counts_doc_values):\n",
    "            print(f\"Jaar {year_fc} (documenten): Voorspeld aantal = {count_fc:.1f}\")\n",
    "    else:\n",
    "        print(\"Niet genoeg data voor documenten voorspelling.\")\n",
    "else:\n",
    "    print(\"Geen historische data geladen voor documenten voorspelling (plot 3).\")\n",
    "\n",
    "# Artikelen\n",
    "print(f\"\\n--- Voorspellingsdetails Artikelen (Historisch: {HISTORICAL_START_ART}-{HISTORICAL_END_ART}, Prognose: {FORECAST_START_ART}-{FORECAST_END_ART}) ---\")\n",
    "articles_by_year_print = {}\n",
    "# Herlaad data specifiek voor de historische periode van artikelen\n",
    "for file_path in glob.glob(os.path.join(DATA_DIRECTORY, \"*.json\")):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f: data = json.load(f)\n",
    "        document = data.get(\"Document\", {}); bs_datum = document.get(\"BSDatum\")\n",
    "        if not bs_datum: continue\n",
    "        year = int(bs_datum[:4])\n",
    "        if HISTORICAL_START_ART <= year <= HISTORICAL_END_ART: # Gebruik HISTORICAL_START_ART en HISTORICAL_END_ART\n",
    "            articles_by_year_print[year] = articles_by_year_print.get(year, 0) + count_articles_in_document(data)\n",
    "    except Exception: continue\n",
    "\n",
    "if articles_by_year_print:\n",
    "    series_art_print = pd.Series(articles_by_year_print).sort_index()\n",
    "    if not series_art_print.empty and len(series_art_print.index) >=2:\n",
    "        x_art = np.array(series_art_print.index); y_art = np.array(series_art_print.values)\n",
    "        slope_art, intercept_art = np.polyfit(x_art, y_art, 1)\n",
    "        future_years_art_range = np.arange(FORECAST_START_ART, FORECAST_END_ART + 1)\n",
    "        forecast_counts_art_values = slope_art * future_years_art_range + intercept_art\n",
    "        forecast_counts_art_values[forecast_counts_art_values < 0] = 0\n",
    "        for year_fc, count_fc in zip(future_years_art_range, forecast_counts_art_values):\n",
    "            print(f\"Jaar {year_fc} (artikelen): Voorspeld aantal = {count_fc:.1f}\")\n",
    "    else:\n",
    "        print(\"Niet genoeg data voor artikelen voorspelling.\")\n",
    "else:\n",
    "    print(\"Geen historische data voor artikelen voorspelling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorteren van de Vlaamse Codex\n",
    "\n",
    "Het organiseren van de verschillende wetgevingsdocumenten is makkelijk. We hebben ‘decreet’ verder opgesplitst voor 'decreet' in heeft inhoud = true en false. Momenteel zijn er 2359 decreten die geldig zijn. Deze zijn te vinden in de **map O_type_decreet** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# --- Configuratie ---\n",
    "# Bronmap met de originele JSON-bestanden\n",
    "source_dir = 'codexjson'\n",
    "# Doelmap waar de georganiseerde bestanden komen\n",
    "target_base_dir = 'organized_documents_v3' # Nieuwe naam om conflicten te vermijden\n",
    "\n",
    "# Speciale behandeling voor het type \"Decreet\"\n",
    "DECREET_TYPE_NAME = \"Decreet\"\n",
    "DECREET_FOLDER_PREFIX = \"0_Type_\" # Prefix om \"Decreet\" bovenaan te sorteren\n",
    "\n",
    "# --- Stap 1: Organiseer alle documenten op basis van WetgevingDocumentType ---\n",
    "\n",
    "print(f\"--- Start Stap 1: Organiseren op Documenttype ---\")\n",
    "os.makedirs(target_base_dir, exist_ok=True)\n",
    "print(f\"Hoofddoelmap '{target_base_dir}' is gereed.\")\n",
    "\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        source_file_path = os.path.join(source_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            with open(source_file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            document_type_value = None\n",
    "            if 'Document' in data and isinstance(data.get('Document'), dict):\n",
    "                document_type_value = data['Document'].get('WetgevingDocumentType')\n",
    "\n",
    "            if isinstance(document_type_value, str) and document_type_value.strip():\n",
    "                folder_name_from_type = document_type_value.strip()\n",
    "            else:\n",
    "                folder_name_from_type = \"Onbekend_Type\"\n",
    "                # print(f\"Info (Stap 1): Bestand {filename} - 'WetgevingDocumentType' onbekend. Gebruikt 'Onbekend_Type'.\")\n",
    "\n",
    "            sanitized_folder_name_base = re.sub(r'[\\\\/*?:\"<>|]', '_', folder_name_from_type)\n",
    "            \n",
    "            if folder_name_from_type == DECREET_TYPE_NAME:\n",
    "                final_folder_name = f\"{DECREET_FOLDER_PREFIX}{sanitized_folder_name_base}\"\n",
    "            else:\n",
    "                final_folder_name = sanitized_folder_name_base # Geen prefix voor andere types\n",
    "\n",
    "            target_subdir = os.path.join(target_base_dir, final_folder_name)\n",
    "            os.makedirs(target_subdir, exist_ok=True)\n",
    "            \n",
    "            target_file_path = os.path.join(target_subdir, filename)\n",
    "            shutil.copy2(source_file_path, target_file_path)\n",
    "            # print(f\"Gekopieerd (Stap 1): '{source_file_path}' naar '{target_file_path}'\")\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Fout (Stap 1): Kon JSON niet decoderen uit {source_file_path}. Bestand overgeslagen.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fout (Stap 1): Een onverwachte fout trad op bij het verwerken van {source_file_path}: {e}. Bestand overgeslagen.\")\n",
    "\n",
    "print(f\"\\n--- Stap 1 Voltooid: Bestandsorganisatie op type in '{target_base_dir}' ---\")\n",
    "\n",
    "# --- Stap 2: Splits de \"Decreet\" map verder op basis van 'HeeftInhoud' ---\n",
    "# We zullen verder werken met de inhoud van de map \"0_Type_Decreet\", \n",
    "# met name de submappen 'HeeftInhoud_True' en 'HeeftInhoud_False'.\n",
    "\n",
    "print(f\"\\n--- Start Stap 2: Opsplitsen van '{DECREET_TYPE_NAME}' map ---\")\n",
    "\n",
    "sanitized_decreet_base_name = re.sub(r'[\\\\/*?:\"<>|]', '_', DECREET_TYPE_NAME)\n",
    "decreet_main_folder_path = os.path.join(target_base_dir, f\"{DECREET_FOLDER_PREFIX}{sanitized_decreet_base_name}\")\n",
    "\n",
    "if not os.path.isdir(decreet_main_folder_path):\n",
    "    print(f\"Info (Stap 2): De map '{decreet_main_folder_path}' bestaat niet. Mogelijk waren er geen documenten van het type '{DECREET_TYPE_NAME}'. Stap 2 overgeslagen.\")\n",
    "else:\n",
    "    true_dir = os.path.join(decreet_main_folder_path, 'HeeftInhoud_True')\n",
    "    false_dir = os.path.join(decreet_main_folder_path, 'HeeftInhoud_False')\n",
    "\n",
    "    os.makedirs(true_dir, exist_ok=True)\n",
    "    os.makedirs(false_dir, exist_ok=True)\n",
    "    print(f\"Doelmappen voor 'HeeftInhoud' in '{decreet_main_folder_path}' zijn aangemaakt/geverifieerd.\")\n",
    "\n",
    "    files_to_move = []\n",
    "    for item_name in os.listdir(decreet_main_folder_path):\n",
    "        source_item_path = os.path.join(decreet_main_folder_path, item_name)\n",
    "        if os.path.isfile(source_item_path) and item_name.endswith('.json'):\n",
    "            files_to_move.append((item_name, source_item_path))\n",
    "\n",
    "    for item_name, source_item_path in files_to_move:\n",
    "        try:\n",
    "            with open(source_item_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            heeft_inhoud_value = None\n",
    "            document_data = data.get('Document')\n",
    "            moved = False\n",
    "\n",
    "            if isinstance(document_data, dict):\n",
    "                heeft_inhoud_value = document_data.get('HeeftInhoud')\n",
    "\n",
    "            if isinstance(heeft_inhoud_value, bool):\n",
    "                if heeft_inhoud_value: # True\n",
    "                    target_file_path = os.path.join(true_dir, item_name)\n",
    "                    shutil.move(source_item_path, target_file_path)\n",
    "                    # print(f\"Verplaatst (Stap 2): '{item_name}' naar '{os.path.basename(true_dir)}'\")\n",
    "                    moved = True\n",
    "                else: # False\n",
    "                    target_file_path = os.path.join(false_dir, item_name)\n",
    "                    shutil.move(source_item_path, target_file_path)\n",
    "                    # print(f\"Verplaatst (Stap 2): '{item_name}' naar '{os.path.basename(false_dir)}'\")\n",
    "                    moved = True\n",
    "            \n",
    "            if not moved:\n",
    "                # Als het bestand niet verplaatst is (omdat HeeftInhoud geen boolean was of ontbrak),\n",
    "                # blijft het in de '0_Type_Decreet' map staan.\n",
    "                print(f\"Info (Stap 2): Bestand {item_name} - 'HeeftInhoud' is geen boolean (waarde: '{heeft_inhoud_value}') of 'Document' structuur is onverwacht. Bestand blijft in '{decreet_main_folder_path}'.\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Fout (Stap 2): Kon JSON niet decoderen uit {source_item_path}. Bestand blijft in '{decreet_main_folder_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fout (Stap 2): Onverwachte fout bij {source_item_path}: {e}. Bestand blijft in '{decreet_main_folder_path}'.\")\n",
    "            \n",
    "    print(f\"\\n--- Stap 2 Voltooid: Herschikken van '{DECREET_TYPE_NAME}' map ---\")\n",
    "    print(f\"De '{DECREET_TYPE_NAME}' bestanden zijn nu, indien mogelijk, onderverdeeld in '{os.path.basename(true_dir)}' en '{os.path.basename(false_dir)}' binnen '{decreet_main_folder_path}'.\")\n",
    "    print(f\"Bestanden met onduidelijke 'HeeftInhoud' status blijven in de hoofmap '{decreet_main_folder_path}'.\")\n",
    "\n",
    "print(\"\\nAlle bestandsorganisatie voltooid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML scrapen van alle decreten met heeftinhoud_true\n",
    "\n",
    "Scraper die alle geldige decreten via de website van de Vlaamse Codex ophaalt. Dit gebeurt per geldig decreet, voor zowel de versie met als zonder annotaties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import aiohttp \n",
    "import nest_asyncio \n",
    "import time \n",
    "\n",
    "nest_asyncio.apply() \n",
    "\n",
    "# --- Configuratie ---\n",
    "# Pad naar de map met JSON-bestanden van decreten die inhoud hebben\n",
    "SOURCE_JSON_DIR = os.path.join('organized_documents_v3', '0_Type_Decreet', 'HeeftInhoud_True')\n",
    "\n",
    "# Hoofddoelmap voor de gedownloade HTML-bestanden\n",
    "TARGET_HTML_BASE_DIR = os.path.join('organized_documents_v3', '0_Type_Decreet', 'HTML_Bestanden')\n",
    "TARGET_HTML_WITH_ANNOTATIONS_DIR = os.path.join(TARGET_HTML_BASE_DIR, 'Met_Annotaties')\n",
    "TARGET_HTML_WITHOUT_ANNOTATIONS_DIR = os.path.join(TARGET_HTML_BASE_DIR, 'Zonder_Annotaties')\n",
    "\n",
    "# Basis URL voor het downloaden van de HTML-documenten\n",
    "BASE_DOWNLOAD_URL = \"https://codex.vlaanderen.be/PrintDocument.ashx\"\n",
    "\n",
    "# Parameters voor asynchrone downloads\n",
    "CONCURRENT_DOWNLOADS = 25\n",
    "REQUEST_TIMEOUT = 60\n",
    "RETRY_ATTEMPTS = 2\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "# --- Helperfuncties ---\n",
    "\n",
    "def extract_id_from_filename(filename):\n",
    "    \"\"\"Extraheert het document-ID uit de bestandsnaam (bv. document_12345_raw.json -> 12345).\"\"\"\n",
    "    match = re.match(r\"document_(\\d+)_raw\\.json\", filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "async def fetch_html_with_retries(session, doc_id, geannoteerd_value, filename_for_log):\n",
    "    \"\"\"Downloadt één HTML-versie asynchroon met nieuwe pogingen.\"\"\"\n",
    "    params = {\n",
    "        'id': doc_id,\n",
    "        'datum': '',\n",
    "        'geannoteerd': str(geannoteerd_value).lower(),\n",
    "        'print': 'false'\n",
    "    }\n",
    "    # Definieer een timeout object voor de request\n",
    "    timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT, connect=20) # bv. 20s voor connect, rest voor read\n",
    "\n",
    "    for attempt in range(RETRY_ATTEMPTS + 1):\n",
    "        try:\n",
    "            async with session.get(BASE_DOWNLOAD_URL, params=params, timeout=timeout) as response:\n",
    "                response.raise_for_status()\n",
    "                return await response.text()\n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"Timeout (poging {attempt + 1}/{RETRY_ATTEMPTS + 1}) ID {doc_id} (annot={geannoteerd_value}) ({filename_for_log})\")\n",
    "            if attempt < RETRY_ATTEMPTS:\n",
    "                await asyncio.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                return None\n",
    "        except aiohttp.ClientResponseError as http_err:\n",
    "            print(f\"HTTP fout {http_err.status} (poging {attempt + 1}/{RETRY_ATTEMPTS + 1}) ID {doc_id} (annot={geannoteerd_value}) ({filename_for_log}): {http_err.message}\")\n",
    "            if attempt < RETRY_ATTEMPTS and http_err.status != 404: # Niet opnieuw proberen bij 404 (Not Found)\n",
    "                await asyncio.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                return None\n",
    "        except aiohttp.ClientConnectionError as conn_err:\n",
    "            print(f\"Connectie fout (poging {attempt + 1}/{RETRY_ATTEMPTS + 1}) ID {doc_id} (annot={geannoteerd_value}) ({filename_for_log}): {conn_err}\")\n",
    "            if attempt < RETRY_ATTEMPTS:\n",
    "                await asyncio.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Algemene fout (poging {attempt + 1}/{RETRY_ATTEMPTS + 1}) ID {doc_id} (annot={geannoteerd_value}) ({filename_for_log}): {e}\")\n",
    "            if attempt < RETRY_ATTEMPTS:\n",
    "                await asyncio.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "async def process_single_document(session, doc_id, geannoteerd_value, original_filename):\n",
    "    \"\"\"Verwerkt het downloaden en opslaan voor één document en één annotatie-instelling.\"\"\"\n",
    "    html_content = await fetch_html_with_retries(session, doc_id, geannoteerd_value, original_filename)\n",
    "    if html_content:\n",
    "        annot_suffix = \"met_annotaties\" if geannoteerd_value else \"zonder_annotaties\"\n",
    "        html_filename = f\"document_{doc_id}_{annot_suffix}.html\"\n",
    "        \n",
    "        target_dir = TARGET_HTML_WITH_ANNOTATIONS_DIR if geannoteerd_value else TARGET_HTML_WITHOUT_ANNOTATIONS_DIR\n",
    "        html_file_path = os.path.join(target_dir, html_filename)\n",
    "            \n",
    "        try:\n",
    "            with open(html_file_path, 'w', encoding='utf-8') as f_html:\n",
    "                f_html.write(html_content)\n",
    "            # print(f\"Opgeslagen: '{html_file_path}' (ID: {doc_id}, Annotaties: {geannoteerd_value})\") \n",
    "        except IOError as io_err:\n",
    "            print(f\"IO Fout bij opslaan {html_file_path}: {io_err}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Onverwachte fout bij opslaan {html_file_path}: {e}\")\n",
    "\n",
    "async def download_and_save_html_versions():\n",
    "    \"\"\"\n",
    "    Doorloopt JSON-bestanden, extraheert ID's, downloadt asynchroon twee HTML-versies\n",
    "    (met en zonder annotaties) en slaat deze op.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(SOURCE_JSON_DIR):\n",
    "        print(f\"Fout: De bronmap '{SOURCE_JSON_DIR}' met JSON-bestanden is niet gevonden.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(TARGET_HTML_WITH_ANNOTATIONS_DIR, exist_ok=True)\n",
    "    os.makedirs(TARGET_HTML_WITHOUT_ANNOTATIONS_DIR, exist_ok=True)\n",
    "    print(f\"Doelmappen in '{TARGET_HTML_BASE_DIR}' zijn aangemaakt/geverifieerd.\")\n",
    "    print(f\"Starten met het downloaden van HTML-versies voor documenten in '{SOURCE_JSON_DIR}'...\")\n",
    "\n",
    "    doc_ids_to_process = []\n",
    "    for filename in os.listdir(SOURCE_JSON_DIR):\n",
    "        if filename.endswith(\".json\"):\n",
    "            doc_id = extract_id_from_filename(filename)\n",
    "            if not doc_id:\n",
    "                print(f\"Waarschuwing: Kon geen ID extraheren uit bestandsnaam '{filename}'. Overgeslagen.\")\n",
    "                continue\n",
    "            doc_ids_to_process.append((doc_id, filename))\n",
    "\n",
    "    if not doc_ids_to_process:\n",
    "        print(\"Geen JSON-bestanden gevonden om te verwerken.\")\n",
    "        return\n",
    "\n",
    "    # ssl=False kan soms helpen met SSL-certificaat problemen.\n",
    "    # force_close=True kan helpen om verbindingen sneller te sluiten na gebruik.\n",
    "    connector = aiohttp.TCPConnector(limit_per_host=CONCURRENT_DOWNLOADS, ssl=False, force_close=True)\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        tasks = []\n",
    "        for i, (doc_id, original_filename) in enumerate(doc_ids_to_process):\n",
    "            tasks.append(asyncio.create_task(\n",
    "                process_single_document(session, doc_id, True, original_filename)\n",
    "            ))\n",
    "            tasks.append(asyncio.create_task(\n",
    "                process_single_document(session, doc_id, False, original_filename)\n",
    "            ))\n",
    "            \n",
    "            # Beheer batchgrootte om niet te veel taken tegelijk in geheugen te hebben en de server niet te overspoelen\n",
    "            if len(tasks) >= CONCURRENT_DOWNLOADS * 2 : # Wacht op een batch (aangezien we 2 taken per ID maken)\n",
    "                print(f\"Batch {i // CONCURRENT_DOWNLOADS + 1} gestart, wachten op voltooiing...\")\n",
    "                await asyncio.gather(*tasks)\n",
    "                tasks = [] # Reset takenlijst voor volgende batch\n",
    "                print(f\"Batch voltooid, kleine pauze...\")\n",
    "                await asyncio.sleep(0.5) # Iets langere pauze tussen batches\n",
    "        \n",
    "        if tasks: # Verwerk resterende taken\n",
    "            print(\"Verwerken van resterende taken...\")\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"\\nDownloaden van HTML-versies voltooid.\")\n",
    "\n",
    "# --- Hoofduitvoering aangepast voor Jupyter Notebook ---\n",
    "async def main_async_download_script(): \n",
    "    start_time = time.time()\n",
    "    await download_and_save_html_versions()\n",
    "    end_time = time.time()\n",
    "    print(f\"Totale downloadtijd: {end_time - start_time:.2f} seconden.\")\n",
    "\n",
    "# Roep de hoofdfunctie aan. Dankzij nest_asyncio.apply() bovenaan,\n",
    "# zou asyncio.run() nu moeten werken in Jupyter.\n",
    "if __name__ == '__main__': # Deze check is nuttig als je het script ook buiten Jupyter wilt runnen\n",
    "    asyncio.run(main_async_download_script())\n",
    "else: # Voor directe uitvoering in Jupyter cel\n",
    "    asyncio.run(main_async_download_script())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML naar markdown\n",
    "\n",
    "Deze cell converteert automatisch alle HTML-bestanden uit de mappen \"Met_Annotaties\" en \"Zonder_Annotaties\" naar Markdown-bestanden met behulp MarkItDown. Door gebruik te maken van parallelle verwerking worden bestanden tegelijk omgezet, wat het proces aanzienlijk versnelt. De geconverteerde Markdown-bestanden worden overzichtelijk opgeslagen in aparte doelmappen per type annotatie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# Definieer de bronmappen met de HTML-bestanden\n",
    "SOURCE_HTML_DIRS = {\n",
    "    'met_annotaties': os.path.join('organized_documents_v3', '0_Type_Decreet', 'HTML_Bestanden', 'Met_Annotaties'),\n",
    "    'zonder_annotaties': os.path.join('organized_documents_v3', '0_Type_Decreet', 'HTML_Bestanden', 'Zonder_Annotaties')\n",
    "}\n",
    "\n",
    "# Definieer de doelmappen voor de geconverteerde Markdown-bestanden\n",
    "TARGET_MD_DIRS = {\n",
    "    'met_annotaties': os.path.join('organized_documents_v3', '0_Type_Decreet', 'Markdown_Bestanden', 'Met_Annotaties'),\n",
    "    'zonder_annotaties': os.path.join('organized_documents_v3', '0_Type_Decreet', 'Markdown_Bestanden', 'Zonder_Annotaties')\n",
    "}\n",
    "\n",
    "# PERFORMANCE INSTELLINGEN\n",
    "MAX_WORKERS = 8  # Aantal parallelle processen \n",
    "BATCH_SIZE = 50  # Aantal bestanden per batch voor voortgangsrapportage\n",
    "\n",
    "# MarkItDown executable configuratie\n",
    "MARKITDOWN_EXECUTABLE = \"markitdown\"\n",
    "\n",
    "# Maak de doelmappen aan als deze nog niet bestaan\n",
    "for target_dir in TARGET_MD_DIRS.values():\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(f\"Doelmap '{target_dir}' is aangemaakt/geverifieerd.\")\n",
    "\n",
    "def extract_id_from_html_filename(filename):\n",
    "    \"\"\"Extraheert het document-ID uit de HTML-bestandsnaam.\"\"\"\n",
    "    match = re.match(r\"document_(\\d+)_(met|zonder)_annotaties\\.html\", filename)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2)\n",
    "    return None, None\n",
    "\n",
    "def process_single_file(file_info):\n",
    "    \"\"\"\n",
    "    Verwerkt een enkel HTML-bestand naar Markdown.\n",
    "    Deze functie wordt parallel uitgevoerd.\n",
    "    \"\"\"\n",
    "    source_html_path, target_md_path, filename, doc_id = file_info\n",
    "    \n",
    "    # Skip als output bestand al bestaat en niet leeg is\n",
    "    if os.path.exists(target_md_path) and os.path.getsize(target_md_path) > 0:\n",
    "        return {\n",
    "            'filename': filename,\n",
    "            'doc_id': doc_id,\n",
    "            'status': 'skipped',\n",
    "            'message': 'Bestand bestaat al',\n",
    "            'size': os.path.getsize(target_md_path)\n",
    "        }\n",
    "    \n",
    "    # Stel het commando samen\n",
    "    if isinstance(MARKITDOWN_EXECUTABLE, list):\n",
    "        command = [*MARKITDOWN_EXECUTABLE, source_html_path, \"-o\", target_md_path]\n",
    "    else:\n",
    "        command = [MARKITDOWN_EXECUTABLE, source_html_path, \"-o\", target_md_path]\n",
    "    \n",
    "    try:\n",
    "        # Voer markitdown uit met timeout om hangende processen te voorkomen\n",
    "        process = subprocess.run(\n",
    "            command, \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            encoding='utf-8', \n",
    "            check=False,\n",
    "            timeout=60  # 60 seconden timeout per bestand\n",
    "        )\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            if os.path.exists(target_md_path) and os.path.getsize(target_md_path) > 0:\n",
    "                file_size = os.path.getsize(target_md_path)\n",
    "                return {\n",
    "                    'filename': filename,\n",
    "                    'doc_id': doc_id,\n",
    "                    'status': 'success',\n",
    "                    'message': f'Succesvol geconverteerd ({file_size} bytes)',\n",
    "                    'size': file_size\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'filename': filename,\n",
    "                    'doc_id': doc_id,\n",
    "                    'status': 'error',\n",
    "                    'message': 'Output bestand leeg of niet aangemaakt',\n",
    "                    'stdout': process.stdout[:200] if process.stdout else ''\n",
    "                }\n",
    "        else:\n",
    "            return {\n",
    "                'filename': filename,\n",
    "                'doc_id': doc_id,\n",
    "                'status': 'error',\n",
    "                'message': f'Return code: {process.returncode}',\n",
    "                'stderr': process.stderr[:200] if process.stderr else '',\n",
    "                'stdout': process.stdout[:200] if process.stdout else ''\n",
    "            }\n",
    "    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\n",
    "            'filename': filename,\n",
    "            'doc_id': doc_id,\n",
    "            'status': 'error',\n",
    "            'message': 'Timeout na 60 seconden'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'filename': filename,\n",
    "            'doc_id': doc_id,\n",
    "            'status': 'error',\n",
    "            'message': f'Onverwachte fout: {str(e)}'\n",
    "        }\n",
    "\n",
    "def prepare_file_list():\n",
    "    \"\"\"Bereidt een lijst voor van alle te verwerken bestanden.\"\"\"\n",
    "    all_files = []\n",
    "    \n",
    "    for folder_type, source_dir in SOURCE_HTML_DIRS.items():\n",
    "        if not os.path.exists(source_dir):\n",
    "            print(f\"Waarschuwing: De bronmap '{source_dir}' is niet gevonden. Overgeslagen.\")\n",
    "            continue\n",
    "        \n",
    "        target_dir = TARGET_MD_DIRS[folder_type]\n",
    "        \n",
    "        for filename in os.listdir(source_dir):\n",
    "            if filename.endswith(\".html\"):\n",
    "                doc_id, annotation_type = extract_id_from_html_filename(filename)\n",
    "                \n",
    "                if not doc_id:\n",
    "                    print(f\"Waarschuwing: Kon geen ID extraheren uit '{filename}'. Overgeslagen.\")\n",
    "                    continue\n",
    "                \n",
    "                source_html_path = os.path.join(source_dir, filename)\n",
    "                md_filename = f\"document_{doc_id}_{annotation_type}_annotaties_converted.md\"\n",
    "                target_md_path = os.path.join(target_dir, md_filename)\n",
    "                \n",
    "                all_files.append((source_html_path, target_md_path, filename, doc_id))\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "def convert_html_to_markdown_parallel():\n",
    "    \"\"\"\n",
    "    Parallelle versie van de HTML naar Markdown conversie.\n",
    "    \"\"\"\n",
    "    # Controleer of markitdown beschikbaar is\n",
    "    executable_to_check = MARKITDOWN_EXECUTABLE if isinstance(MARKITDOWN_EXECUTABLE, str) else MARKITDOWN_EXECUTABLE[0]\n",
    "    if not shutil.which(executable_to_check):\n",
    "        print(f\"FOUT: Het commando '{executable_to_check}' kon niet worden gevonden.\")\n",
    "        print(f\"Zorg ervoor dat 'markitdown' correct is geïnstalleerd.\")\n",
    "        return\n",
    "    \n",
    "    # Bereid lijst van bestanden voor\n",
    "    print(\"Voorbereiden van bestandslijst...\")\n",
    "    all_files = prepare_file_list()\n",
    "    \n",
    "    if not all_files:\n",
    "        print(\"Geen bestanden gevonden om te verwerken.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Gevonden {len(all_files)} bestanden om te verwerken.\")\n",
    "    print(f\"Gebruik {MAX_WORKERS} parallelle workers.\")\n",
    "    \n",
    "    # Statistieken\n",
    "    total_converted = 0\n",
    "    total_errors = 0\n",
    "    total_skipped = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Parallelle verwerking met ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit alle taken\n",
    "        future_to_file = {executor.submit(process_single_file, file_info): file_info for file_info in all_files}\n",
    "        \n",
    "        # Verwerk resultaten zodra ze beschikbaar komen\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(future_to_file)):\n",
    "            result = future.result()\n",
    "            \n",
    "            # Update statistieken\n",
    "            if result['status'] == 'success':\n",
    "                total_converted += 1\n",
    "                total_size += result.get('size', 0)\n",
    "                print(f\"✓ {result['filename']} - {result['message']}\")\n",
    "            elif result['status'] == 'skipped':\n",
    "                total_skipped += 1\n",
    "                total_size += result.get('size', 0)\n",
    "                print(f\"⏭ {result['filename']} - {result['message']}\")\n",
    "            else:\n",
    "                total_errors += 1\n",
    "                print(f\"✗ {result['filename']} - {result['message']}\")\n",
    "                if 'stderr' in result and result['stderr']:\n",
    "                    print(f\"    Stderr: {result['stderr']}\")\n",
    "            \n",
    "            # Voortgangsrapportage per batch\n",
    "            if (i + 1) % BATCH_SIZE == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = (i + 1) / elapsed\n",
    "                eta = (len(all_files) - i - 1) / rate if rate > 0 else 0\n",
    "                print(f\"\\n📊 Voortgang: {i + 1}/{len(all_files)} ({(i + 1)/len(all_files)*100:.1f}%)\")\n",
    "                print(f\"⏱️  Snelheid: {rate:.1f} bestanden/sec, ETA: {eta:.0f} seconden\\n\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Eindrapport\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🎉 CONVERSIE VOLTOOID!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"📈 STATISTIEKEN:\")\n",
    "    print(f\"   ✅ Succesvol geconverteerd: {total_converted}\")\n",
    "    print(f\"   ⏭️  Overgeslagen (bestond al): {total_skipped}\")\n",
    "    print(f\"   ❌ Fouten: {total_errors}\")\n",
    "    print(f\"   📁 Totaal verwerkt: {total_converted + total_errors + total_skipped}\")\n",
    "    print(f\"   💾 Totale output grootte: {total_size / (1024*1024):.1f} MB\")\n",
    "    print(f\"\")\n",
    "    print(f\"⏱️  PERFORMANCE:\")\n",
    "    print(f\"   🕐 Totale tijd: {total_time:.1f} seconden\")\n",
    "    print(f\"   🚀 Gemiddelde snelheid: {len(all_files)/total_time:.1f} bestanden/sec\")\n",
    "    print(f\"   ⚡ Versnelling t.o.v. sequentieel: ~{MAX_WORKERS}x\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_html_to_markdown_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown cleaning\n",
    "\n",
    "Het omzetten van HTML naar Markdown zorgde ervoor dat oa. haakjes niet correct stonden. Dit is hier grotendeels mee opgelost. Er zullen nog edgcases zitten en de vormgeving gaat allicht niet 100 procent correct zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Definieer de bron- en doelmappen\n",
    "SOURCE_MD_DIR = os.path.join('organized_documents_v3', '0_Type_Decreet', 'Markdown_Bestanden')\n",
    "TARGET_MD_DIR = os.path.join('organized_documents_v3', '0_Type_Decreet', 'Markdown_Bestanden_Cleaned')\n",
    "\n",
    "def clean_markdown_formatting(content):\n",
    "    \"\"\"\n",
    "    Zet de vormgeving van artikelen recht in Markdown-bestanden.\n",
    "    \n",
    "    Transformeert alle varianten van artikelnummering:\n",
    "    - Artikel X.\n",
    "    - Artikel Xbis, Artikel Xter, etc.\n",
    "    - Artikel X/Y (met slash)\n",
    "    - Artikel Xbis/Y\n",
    "    - Artikel Xter decies (met spatie)\n",
    "    - Artikel Xquater decies\n",
    "    \n",
    "    En ruimt ook losse haakjes-structuren op.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Uitgebreide Latijnse suffixen\n",
    "    latin_suffixes = r'(?:bis|ter|quater|quinquies|sexies|septies|octies|nonies|decies|undecies|duodecies)'\n",
    "    \n",
    "    # STAP 1: Complexe pattern die alle mogelijke artikel-varianten ondersteunt\n",
    "    article_pattern = rf'''\n",
    "        (Artikel\\s+                           # \"Artikel \"\n",
    "         \\d+                                  # Basis nummer (bijv. \"17\")\n",
    "         (?:{latin_suffixes})?                # Optioneel eerste Latijns suffix (bijv. \"bis\")\n",
    "         (?:/\\d+)?                           # Optioneel slash + nummer (bijv. \"/1\")\n",
    "         (?:\\s+{latin_suffixes})?            # Optioneel spatie + tweede Latijns suffix (bijv. \" decies\")\n",
    "         \\.)                                 # Punt\n",
    "         \\s*\\n+\\s*                           # Whitespace en newlines\n",
    "         \\(                                  # Opening haakje\n",
    "         \\s*\\n+                              # Whitespace en newlines\n",
    "         (.*?)                               # Eerste deel (datum of ...)\n",
    "         \\n+\\s*                              # Whitespace en newlines\n",
    "         -                                   # Streepje\n",
    "         \\s*\\n+                              # Whitespace en newlines\n",
    "         (.*?)                               # Tweede deel (meestal ...)\n",
    "         \\n+\\s*                              # Whitespace en newlines\n",
    "         \\)                                  # Sluitend haakje\n",
    "    '''\n",
    "    \n",
    "    def replace_article(match):\n",
    "        article_title = match.group(1)  # Volledige artikel titel\n",
    "        first_part = match.group(2).strip()  # Eerste deel tussen haakjes\n",
    "        second_part = match.group(3).strip()  # Tweede deel na de streep\n",
    "        \n",
    "        # Maak de nieuwe formatting\n",
    "        return f\"**{article_title}**({first_part}-{second_part})\"\n",
    "    \n",
    "    # Vervang alle artikel-matches\n",
    "    cleaned_content = re.sub(article_pattern, replace_article, content, flags=re.DOTALL | re.MULTILINE | re.VERBOSE)\n",
    "    \n",
    "    # STAP 2: Ruim losse haakjes-structuren op (die niet onder een artikel staan)\n",
    "    # Pattern voor losse haakjes-structuren\n",
    "    loose_brackets_pattern = r'''\n",
    "        (?<!\\*\\*)                            # Niet voorafgegaan door ** (dus niet al verwerkt)\n",
    "        \\n\\s*                                # Newline + optionele whitespace\n",
    "        \\(                                   # Opening haakje\n",
    "        \\s*\\n+                               # Whitespace en newlines\n",
    "        (.*?)                                # Eerste deel (datum of ...)\n",
    "        \\n+\\s*                               # Whitespace en newlines\n",
    "        -                                    # Streepje\n",
    "        \\s*\\n+                               # Whitespace en newlines\n",
    "        (.*?)                                # Tweede deel (meestal ...)\n",
    "        \\n+\\s*                               # Whitespace en newlines\n",
    "        \\)                                   # Sluitend haakje\n",
    "    '''\n",
    "    \n",
    "    def replace_loose_brackets(match):\n",
    "        first_part = match.group(1).strip()\n",
    "        second_part = match.group(2).strip()\n",
    "        return f\"\\n({first_part}-{second_part})\"\n",
    "    \n",
    "    # Vervang losse haakjes-structuren\n",
    "    cleaned_content = re.sub(loose_brackets_pattern, replace_loose_brackets, cleaned_content, flags=re.DOTALL | re.MULTILINE | re.VERBOSE)\n",
    "    \n",
    "    return cleaned_content\n",
    "\n",
    "def process_markdown_file(source_path, target_path):\n",
    "    \"\"\"Verwerkt een enkel Markdown-bestand.\"\"\"\n",
    "    try:\n",
    "        # Lees het originele bestand\n",
    "        with open(source_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Pas de opmaak aan\n",
    "        cleaned_content = clean_markdown_formatting(content)\n",
    "        \n",
    "        # Schrijf naar het doelbestand\n",
    "        with open(target_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_content)\n",
    "        \n",
    "        return True, \"Succesvol verwerkt\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, f\"Fout: {str(e)}\"\n",
    "\n",
    "def copy_and_clean_markdown_files():\n",
    "    \"\"\"\n",
    "    Kopieert alle Markdown-bestanden en zet de vormgeving recht.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(SOURCE_MD_DIR):\n",
    "        print(f\"FOUT: Bronmap '{SOURCE_MD_DIR}' niet gevonden.\")\n",
    "        return\n",
    "    \n",
    "    # Maak doelmap aan\n",
    "    os.makedirs(TARGET_MD_DIR, exist_ok=True)\n",
    "    print(f\"Doelmap '{TARGET_MD_DIR}' aangemaakt/geverifieerd.\")\n",
    "    \n",
    "    # Submappen\n",
    "    submaps = ['Met_Annotaties', 'Zonder_Annotaties']\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_errors = 0\n",
    "    total_article_replacements = 0\n",
    "    total_bracket_replacements = 0\n",
    "    \n",
    "    for submap in submaps:\n",
    "        source_subdir = os.path.join(SOURCE_MD_DIR, submap)\n",
    "        target_subdir = os.path.join(TARGET_MD_DIR, submap)\n",
    "        \n",
    "        if not os.path.exists(source_subdir):\n",
    "            print(f\"Waarschuwing: Submap '{source_subdir}' niet gevonden. Overgeslagen.\")\n",
    "            continue\n",
    "        \n",
    "        # Maak doelsubmap aan\n",
    "        os.makedirs(target_subdir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nVerwerken van submap: {submap}\")\n",
    "        print(f\"Bron: {source_subdir}\")\n",
    "        print(f\"Doel: {target_subdir}\")\n",
    "        \n",
    "        submap_processed = 0\n",
    "        submap_errors = 0\n",
    "        submap_article_replacements = 0\n",
    "        submap_bracket_replacements = 0\n",
    "        \n",
    "        # Verwerk alle .md bestanden in de submap\n",
    "        for filename in sorted(os.listdir(source_subdir)):\n",
    "            if filename.endswith('.md'):\n",
    "                source_file = os.path.join(source_subdir, filename)\n",
    "                target_file = os.path.join(target_subdir, filename)\n",
    "                \n",
    "                # Tel vervangingen voor dit bestand\n",
    "                with open(source_file, 'r', encoding='utf-8') as f:\n",
    "                    original_content = f.read()\n",
    "                \n",
    "                # Test artikel-matches\n",
    "                latin_suffixes = r'(?:bis|ter|quater|quinquies|sexies|septies|octies|nonies|decies|undecies|duodecies)'\n",
    "                article_test_pattern = rf'''\n",
    "                    (Artikel\\s+\n",
    "                     \\d+\n",
    "                     (?:{latin_suffixes})?\n",
    "                     (?:/\\d+)?\n",
    "                     (?:\\s+{latin_suffixes})?\n",
    "                     \\.)\n",
    "                     \\s*\\n+\\s*\n",
    "                     \\(\n",
    "                     \\s*\\n+\n",
    "                     (.*?)\n",
    "                     \\n+\\s*\n",
    "                     -\n",
    "                     \\s*\\n+\n",
    "                     (.*?)\n",
    "                     \\n+\\s*\n",
    "                     \\)\n",
    "                '''\n",
    "                article_matches = re.findall(article_test_pattern, original_content, flags=re.DOTALL | re.MULTILINE | re.VERBOSE)\n",
    "                \n",
    "                # Test losse haakjes-matches\n",
    "                bracket_test_pattern = r'''\n",
    "                    (?<!\\*\\*)\n",
    "                    \\n\\s*\n",
    "                    \\(\n",
    "                    \\s*\\n+\n",
    "                    (.*?)\n",
    "                    \\n+\\s*\n",
    "                    -\n",
    "                    \\s*\\n+\n",
    "                    (.*?)\n",
    "                    \\n+\\s*\n",
    "                    \\)\n",
    "                '''\n",
    "                bracket_matches = re.findall(bracket_test_pattern, original_content, flags=re.DOTALL | re.MULTILINE | re.VERBOSE)\n",
    "                \n",
    "                file_article_replacements = len(article_matches)\n",
    "                file_bracket_replacements = len(bracket_matches)\n",
    "                \n",
    "                success, message = process_markdown_file(source_file, target_file)\n",
    "                \n",
    "                if success:\n",
    "                    submap_processed += 1\n",
    "                    submap_article_replacements += file_article_replacements\n",
    "                    submap_bracket_replacements += file_bracket_replacements\n",
    "                    \n",
    "                    if file_article_replacements > 0 or file_bracket_replacements > 0:\n",
    "                        print(f\"  ✓ {filename} ({file_article_replacements} artikelen, {file_bracket_replacements} losse haakjes)\")\n",
    "                    else:\n",
    "                        print(f\"  ✓ {filename}\")\n",
    "                else:\n",
    "                    submap_errors += 1\n",
    "                    print(f\"  ✗ {filename} - {message}\")\n",
    "        \n",
    "        print(f\"  Resultaat: {submap_processed} succesvol, {submap_errors} fouten\")\n",
    "        print(f\"  Aangepast: {submap_article_replacements} artikelen, {submap_bracket_replacements} losse haakjes\")\n",
    "        \n",
    "        total_processed += submap_processed\n",
    "        total_errors += submap_errors\n",
    "        total_article_replacements += submap_article_replacements\n",
    "        total_bracket_replacements += submap_bracket_replacements\n",
    "    \n",
    "    # Eindrapport\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MARKDOWN OPSCHONING VOLTOOID\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"📈 TOTAAL OVERZICHT:\")\n",
    "    print(f\"   ✅ Succesvol verwerkt: {total_processed}\")\n",
    "    print(f\"   ❌ Fouten: {total_errors}\")\n",
    "    print(f\"   🔄 Artikelen aangepast: {total_article_replacements}\")\n",
    "    print(f\"   📋 Losse haakjes aangepast: {total_bracket_replacements}\")\n",
    "    print(f\"   📁 Totaal bestanden: {total_processed + total_errors}\")\n",
    "    print(f\"   📂 Opgeslagen in: {TARGET_MD_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    copy_and_clean_markdown_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
